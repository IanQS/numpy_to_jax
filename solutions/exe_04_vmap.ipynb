{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1005520e-0e0c-4f4e-a82e-b5bfbd3049f7",
   "metadata": {},
   "source": [
    "# Jax's vmap\n",
    "\n",
    "## Lesson Goals:\n",
    "\n",
    "By the end of this lesson, you will understand how and where to use `jax`'s `vmap` operation.\n",
    "\n",
    "## Core Concepts:\n",
    "\n",
    "- `vmap`\n",
    "- softmax-regression\n",
    "- gaussian PDF\n",
    "- Neural Network Inference\n",
    "\n",
    "## Concepts In action:\n",
    "\n",
    "- Easy: [lotka-volterra](../case_studies/lotka-volterra/README.md)\n",
    "\n",
    "- Intermediate: [leaky_integrate_and_fire](../case_studies/leaky_integrate_and_fire/README.md)\n",
    " \n",
    "- Advanced: [gaussian_mixture_model](../case_studies/gaussian_mixture_model/README.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d089337-0683-4cc8-9aae-4a0c7443fc32",
   "metadata": {},
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.scipy import stats\n",
    "import numpy as np\n",
    "from jax import vmap\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Vmap\n",
    "\n",
    "`vmap` is a magical little function. You can essentially think of it as applying a function over the first axis of an array i.e. it's a `for-loop` applied\n",
    "to the array. Consider the following exercises where you are doing a simple element-wise addition by 1.\n"
   ],
   "id": "2f93d3bf7350ae8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def custom_vmap(x, func):\n",
    "    return np.asarray([func(_x) for _x in x])\n",
    "\n",
    "def scalar_add(x):\n",
    "    assert len(x.shape) == 0, \"x should be a scalar\"\n",
    "    return x + 1\n",
    "\n",
    "def simple_vmap_example():\n",
    "    \"\"\"\n",
    "    Here, we only have one axis, so applying the function is a one-liner. \n",
    "    TODO: use the `custom_vmap` and `scalar_add` \n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    vec = np.asarray([1, 2, 3, 4])\n",
    "    added_to = custom_vmap(vec, scalar_add) \n",
    "    assert np.all(added_to == vec + 1)\n",
    "    print(\"First-vmap application exercise passed!\")\n",
    "   \n",
    "def less_simple_vmap_example():\n",
    "    \"\"\"\n",
    "    Here, we have two axes that we will map over: the first axis has 3 elements, each of which, is a vector of 5 scalars. You are to \n",
    "    implement the scalar addition once again. \n",
    "    \n",
    "    Hint: think of this as a vmap-on-vmap situation\n",
    "    \"\"\"\n",
    "       \n",
    "    mat = np.random.random(size=(3, 5))\n",
    "    \n",
    "    delayed_add = lambda x: custom_vmap(x, scalar_add)\n",
    "    added_to_mat = custom_vmap(mat, delayed_add) \n",
    "    assert np.all(added_to_mat== mat + 1)\n",
    "    print(\"Second-vmap application exercise passed!\")\n",
    " \n",
    "    \n",
    "    \n",
    "simple_vmap_example()\n",
    "less_simple_vmap_example()"
   ],
   "id": "19ff849652bf0cfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Clearly, the `for-loop` worked, so what's the issue? In frameworks like `numpy`, you want to use vectorized operations i.e. you'd want to just do the `x + 1`, because the CPU can do it all in parallel via SIMD. Comparing vectorized operations and the `for-loop`, you'll see that the vectorized operation is much faster. \n",
    "From a speed perspective, it is clear that vectorized operations are the way to go! Unfortunately, with the vectorized operations, we end up with some unnatural-looking equations i.e. the equations we see in the math vs. our implementation in python will look very different.\n"
   ],
   "id": "dd23ccb14bbaf7fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jax's VMAP\n",
    "\n",
    "The solution is Jax's `vmap`. The `vmap` merges the speed and interpretability! You might want to look at [Jax - automatic vectorization](https://jax.readthedocs.io/en/latest/automatic-vectorization.html) for more information, but Jax will essentially \"add\" the batch axis to the to-be-mapped function. The process is quite similar to what happens when we `jit` a function - in fact, the two are composable! Check out the [gaussian_mixture_model](../case_studies/gaussian_mixture_model/README.md) to see this in action (fair warning, there's quite a bit going on)\n",
    "\n",
    "It's important to note that although\n",
    "\n",
    "> Performance-wise, automatically vectorized code written with vmap often lowers to an identical or near-identical sequence of XLA operations.\n",
    "\n",
    "as per [when to use vmap](https://github.com/jax-ml/jax/discussions/18873), there are occasions where using the `vmap` can be slower, but this is not the expected behavior. If you encounter these situations you should probably open an issue on github!\n"
   ],
   "id": "7e83358f49cd0ab7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Simple Introduction\n",
    "\n",
    "Here we give a quick working introduction to the arguments and what's happening"
   ],
   "id": "ff5bb451c7b22510"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vmap_on_all():\n",
    "    def my_func(_x, _y, _z):\n",
    "        assert len(_x.shape) == 0\n",
    "        assert len(_y.shape) == 0\n",
    "        assert len(_z.shape) == 0\n",
    "        return _x + _y + _z\n",
    "\n",
    "    x = jnp.asarray([1, 2, 3])\n",
    "    y = jnp.asarray([1, 2, 3])\n",
    "    z = jnp.asarray([1, 2, 3])\n",
    "   \n",
    "    # For each argument, we specify the axis to \"map over\"\n",
    "    #   In this example, we essentially do\n",
    "    #   [(1 * 3 ), (2 * 3), (3 * 3)]\n",
    "    vmapped_fn_v1 = vmap(my_func, in_axes=(0, 0, 0))\n",
    "    res = vmapped_fn_v1(x, y, z)\n",
    "    assert jnp.all(\n",
    "        res == \n",
    "        x * 3\n",
    "    )\n",
    "    \n",
    "    # Alternatively, we can choose to not specify the axis\n",
    "    vmapped_fn_v2 = vmap(my_func)\n",
    "    res = vmapped_fn_v2(x, y, z)\n",
    "    assert jnp.all(\n",
    "        res == \n",
    "        x * 3\n",
    "    )\n",
    "    \n",
    "def vmap_broadcast():\n",
    "    def my_func(_x, _y, _z):\n",
    "        assert len(_x.shape) == 0\n",
    "        assert len(_y.shape) == 1 and _y.shape == (3,)\n",
    "        assert len(_z.shape) == 1 and _z.shape == (3,)\n",
    "        return _x * (_y + _z)\n",
    "\n",
    "    x = jnp.asarray([1, 2, 3])\n",
    "    y = jnp.asarray([1, 2, 3])\n",
    "    z = jnp.asarray([1, 2, 3])\n",
    "    \n",
    "    # Here, we specify \"None\" for y and z, which means that they are passed in \"as is\"\n",
    "    vmapped_fn = vmap(my_func, in_axes=(0, None, None))\n",
    "    res = vmapped_fn(x, y, z)\n",
    "    \n",
    "    # What's happening here is we pass in y and z as vectors, so we essentially \"broadcast\" and go up in a dimension\n",
    "    #   [(1 * (1,2,3) * 2), (2 * (1, 2, 3) * 2), (3 * (1, 2, 3) * 2)]\n",
    "    #   = ((2, 4, 6), (4, 8, 12), (6, 12, 18))\n",
    "    assert jnp.all(\n",
    "        res == \n",
    "        jnp.asarray([[2, 4, 6], [4, 8, 12], [6, 12, 18]])\n",
    "    )\n",
    "    assert res.shape == (3, 3)\n",
    "    \n",
    "    \n",
    "    \n",
    "vmap_on_all()\n",
    "vmap_broadcast()"
   ],
   "id": "651a7b7cce769283",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Real World Example: Batched Neural Network Inference\n",
    "\n",
    "And on to some real-world examples! Throughout the rest of the notebook, we'll see the \"core\" function implemented in two ways:\n",
    "\n",
    "`x_vmap` and `x`, where `x` is the way the function would be implemented in vectorized form, and the `x_vmap` is the vmap-ed form. Hopefully this illustrates how using the `vmap` more closely aligns our code with the math.\n",
    "\n",
    "Note: in the first example, we will not see any difference, which is OK! This is more of a warm-up"
   ],
   "id": "d98b0923-f643-43a2-a5c5-cc01c81b6d79"
  },
  {
   "cell_type": "code",
   "id": "0ba6265c-dd7d-4d07-900c-7f70186431ad",
   "metadata": {},
   "source": [
    "# Define the neural network parameters\n",
    "W1 = jnp.array([[0.2, 0.4], [0.5, 0.3]])  # Shape (2, 2)\n",
    "W2 = jnp.array([0.6, 0.7])                # Shape (2,)\n",
    "\n",
    "# Example batched input data\n",
    "X_batch = jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])  # Shape (3, 2)\n",
    "\n",
    "# Activation function\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "# Single forward pass\n",
    "def forward_pass_vmap(x, W1, W2):\n",
    "    \"\"\"\n",
    "    TODO: Your code here\n",
    "    1) Take the dot product between W1 and x\n",
    "    2) apply the relu\n",
    "    3) return the dot product between W2 and the relu result\n",
    "    \"\"\"\n",
    "    assert x.shape == X_batch.shape[1:]\n",
    "    return relu(x.dot(W1)).dot(W2)\n",
    "\n",
    "def forward_pass(X, W1, W2):\n",
    "    return relu(X.dot(W1)).dot(W2)\n",
    "    \n",
    "\n",
    "# Vectorized forward pass using vmap\n",
    "batched_forward_pass = vmap(forward_pass_vmap, in_axes=(0, None, None))\n",
    "vmap_batch_output = batched_forward_pass(X_batch, W1, W2)\n",
    "\n",
    "vectorized_batch_output = forward_pass(X_batch, W1, W2)\n",
    "\n",
    "assert jnp.all(\n",
    "    vmap_batch_output == \n",
    "    vectorized_batch_output\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5e4af4f-422e-45b3-874f-ab1e483533d9",
   "metadata": {},
   "source": [
    "# Calculating the Gaussian PDF\n",
    "\n",
    "`gaussian_pdf_v` is the vmap-ed version of `gaussian_pdf`, which implements the function in vectorized form. You should study how the two are different\n",
    "and how this difference emerges because of the way the data is passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6292b5-f696-4408-9caf-f70066fbc78e",
   "metadata": {},
   "source": [
    "![](../assets/gaussian_pdf.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "60315ea0-0c54-4734-ad0f-d9fa0e06cfe1",
   "metadata": {},
   "source": [
    "mu = np.array([0, 0])  # Mean vector\n",
    "Sigma = np.array([[1, 0], [0, 1]])  # Covariance matrix\n",
    "X = np.array([[1, 1], [2, 2], [3, 3]])  # Point to evaluate the PDF\n",
    "\n",
    "# Arguments implicitly passed in. Done to keep the code cleaner for the example\n",
    "k = mu.shape[0]\n",
    "t1 = (2 * jnp.pi) ** (-k / 2)\n",
    "t2 = jnp.linalg.det(Sigma) ** (-0.5)\n",
    "inv = jnp.linalg.inv(Sigma)\n",
    "\n",
    "def gaussian_pdf_v(x_vec, mu_vec):\n",
    "    \"\"\"\n",
    "    # TODO: implement the single-sample equivalent of `to_exp` in the gaussian_pdf/\n",
    "    #       the elements to be exponentiated in the image above\n",
    "    \"\"\"\n",
    "    diff = x_vec - mu_vec\n",
    "    to_exp = -0.5 * diff.T @ inv @ diff\n",
    "    return t1 * t2 * jnp.exp(to_exp)\n",
    "\n",
    "def gaussian_pdf(x_mat, mu_mat) -> np.array:\n",
    "    diff = x_mat - mu_mat\n",
    "    ###############################################################\n",
    "    to_exp = -0.5 * jnp.sum(diff @ inv * diff, axis=1)\n",
    "    ###############################################################\n",
    "    return t1 * t2 * jnp.exp(to_exp)\n",
    "\n",
    "\n",
    "vmapped_gaussian = vmap(gaussian_pdf_v, in_axes=(0, None))\n",
    "vmap_gauss_res = vmapped_gaussian(X, mu)\n",
    "\n",
    "\n",
    "print(\"VMapped-Gaussian PDF correct?\", jnp.allclose(\n",
    "    vmap_gauss_res, \n",
    "    stats.multivariate_normal.pdf(X, mu, Sigma)\n",
    "))\n",
    "\n",
    "print(\"Typical-Gaussian PDF correct?\", jnp.allclose(\n",
    "    gaussian_pdf(X, mu), \n",
    "    stats.multivariate_normal.pdf(X, mu, Sigma)\n",
    "))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4794415a-102c-4b9b-9553-afb6cece1445",
   "metadata": {},
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b4a24-e885-4741-8ec3-c8b3129ef074",
   "metadata": {},
   "source": [
    "![](../assets/softmax_regression.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "3a81ffd1-6b20-495e-a050-5e9907654135",
   "metadata": {},
   "source": [
    "# Example data\n",
    "X = jnp.array([[1, 2], [2, 3], [3, 4]])  # Batch of inputs\n",
    "W = jnp.array([[0.2, 0.8], [0.5, 0.1]])  # Weight matrix\n",
    "b = jnp.array([0.1, -0.2])  # Bias vector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38488af3-9e0e-48f9-af9f-63c0600ff96b",
   "metadata": {},
   "source": [
    "def softmax_regression(X, W, b):\n",
    "    logits = jnp.dot(X, W) + b\n",
    "    exp_logits = jnp.exp(logits)\n",
    "    return exp_logits / jnp.sum(exp_logits, axis=-1, keepdims=True)\n",
    "\n",
    "# Calculate softmax probabilities for the batch of inputs\n",
    "probabilities = softmax_regression(X, W, b)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5b9e321-84d7-4c6b-8fe6-fe6580134695",
   "metadata": {},
   "source": [
    "def softmax_regression_v(x, W, b):\n",
    "    \"\"\"\n",
    "    TODO: Implement the equivalent of the softmax_regression above on a single row of x\n",
    "    \"\"\"\n",
    "    logits = x.dot(W) + b\n",
    "    exp_logits = jnp.exp(logits)\n",
    "    return exp_logits / jnp.sum(exp_logits)\n",
    "\n",
    "# Vectorize the single input calculation\n",
    "vectorized_softmax_regression = vmap(softmax_regression_v, in_axes=(0, None, None))\n",
    "\n",
    "# Calculate softmax probabilities using vmap\n",
    "probabilities_vmap = vectorized_softmax_regression(X, W, b)\n",
    "print(f\"Vmapped Softmax Regression equal to vectorized?: {np.allclose(probabilities, probabilities_vmap)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93f199f4-b886-4c9a-9b9e-7495c7f21463",
   "metadata": {},
   "source": [
    "# Further Exercises: \n",
    "\n",
    "## 1) Read up on [jax.pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html)\n",
    "\n",
    "`pmap` is a parallel map across devices and is useful for scaling across devices"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e9af2a0-8166-4cef-b538-db7722474301",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
