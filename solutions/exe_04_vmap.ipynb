{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1005520e-0e0c-4f4e-a82e-b5bfbd3049f7",
   "metadata": {},
   "source": [
    "# Jax's vmap\n",
    "\n",
    "## Lesson Goals:\n",
    "\n",
    "By the end of this lesson, you will understand how and where to use `jax`'s `vmap` operation.\n",
    "\n",
    "## Core Concepts:\n",
    "\n",
    "- `vmap`\n",
    "- softmax-regression\n",
    "- gaussian PDF\n",
    "- Neural Network Inference\n",
    "\n",
    "## Concepts In action:\n",
    "\n",
    "- Easy: [lotka-volterra](../case_studies/lotka-volterra/README.md)\n",
    "\n",
    "- Intermediate: [leaky_integrate_and_fire](../case_studies/leaky_integrate_and_fire/README.md)\n",
    " \n",
    "- Advanced: [gaussian_mixture_model](../case_studies/gaussian_mixture_model/README.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d089337-0683-4cc8-9aae-4a0c7443fc32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T01:03:49.555390Z",
     "start_time": "2024-09-26T01:03:49.552391Z"
    }
   },
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.scipy import stats\n",
    "import numpy as np\n",
    "from jax import vmap\n",
    "np.random.seed(42)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Vmap\n",
    "\n",
    "`vmap` is a magical little function. You can essentially think of it as applying a function over the first axis of an array i.e. it's a `for-loop` applied\n",
    "to the array. Consider the following exercises where you are doing a simple element-wise addition by 1.\n"
   ],
   "id": "2f93d3bf7350ae8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T01:03:54.270084Z",
     "start_time": "2024-09-26T01:03:54.266578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_vmap(x, func):\n",
    "    return np.asarray([func(_x) for _x in x])\n",
    "\n",
    "def scalar_add(x):\n",
    "    assert len(x.shape) == 0, \"x should be a scalar\"\n",
    "    return x + 1\n",
    "\n",
    "def simple_vmap_example():\n",
    "    \"\"\"\n",
    "    Here, we only have one axis, so applying the function is a one-liner. \n",
    "    TODO: use the `custom_vmap` and `scalar_add` \n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    vec = np.asarray([1, 2, 3, 4])\n",
    "    added_to = custom_vmap(vec, scalar_add) \n",
    "    assert np.all(added_to == vec + 1)\n",
    "    print(\"First-vmap application exercise passed!\")\n",
    "   \n",
    "def less_simple_vmap_example():\n",
    "    \"\"\"\n",
    "    Here, we have two axes that we will map over: the first axis has 3 elements, each of which, is a vector of 5 scalars. You are to \n",
    "    implement the scalar addition once again. \n",
    "    \n",
    "    Hint: think of this as a vmap-on-vmap situation\n",
    "    \"\"\"\n",
    "       \n",
    "    mat = np.random.random(size=(3, 5))\n",
    "    \n",
    "    delayed_add = lambda x: custom_vmap(x, scalar_add)\n",
    "    added_to_mat = custom_vmap(mat, delayed_add) \n",
    "    assert np.all(added_to_mat== mat + 1)\n",
    "    print(\"Second-vmap application exercise passed!\")\n",
    " \n",
    "    \n",
    "    \n",
    "simple_vmap_example()\n",
    "less_simple_vmap_example()"
   ],
   "id": "19ff849652bf0cfb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-vmap application exercise passed!\n",
      "Second-vmap application exercise passed!\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Clearly, the `for-loop` worked, so what's the issue? In frameworks like `numpy`, you want to use vectorized operations i.e. you'd want to just do the `x + 1`, because the CPU can do it all in parallel via SIMD. Comparing vectorized operations and the `for-loop`, you'll see that the vectorized operation is much faster. \n",
    "From a speed perspective, it is clear that vectorized operations are the way to go! Unfortunately, with the vectorized operations, we end up with some unnatural-looking equations i.e. the equations we see in the math vs. our implementation in python will look very different.\n"
   ],
   "id": "dd23ccb14bbaf7fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Jax's VMAP\n",
    "\n",
    "The solution is Jax's `vmap`. The `vmap` merges the speed and interpretability! You might want to look at [Jax - automatic vectorization](https://jax.readthedocs.io/en/latest/automatic-vectorization.html) for more information, but Jax will essentially \"add\" the batch axis to the to-be-mapped function. The process is quite similar to what happens when we `jit` a function - in fact, the two are composable! Check out the [gaussian_mixture_model](../case_studies/gaussian_mixture_model/README.md) to see this in action (fair warning, there's quite a bit going on)\n"
   ],
   "id": "7e83358f49cd0ab7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Simple Introduction\n",
    "\n",
    "Here we give a quick working introduction to the arguments and what's happening"
   ],
   "id": "ff5bb451c7b22510"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:32:49.988567Z",
     "start_time": "2024-09-26T16:32:49.980155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vmap_on_all():\n",
    "    def my_func(_x, _y, _z):\n",
    "        assert len(_x.shape) == 0\n",
    "        assert len(_y.shape) == 0\n",
    "        assert len(_z.shape) == 0\n",
    "        return _x + _y + _z\n",
    "\n",
    "    x = jnp.asarray([1, 2, 3])\n",
    "    y = jnp.asarray([1, 2, 3])\n",
    "    z = jnp.asarray([1, 2, 3])\n",
    "   \n",
    "    # For each argument, we specify the axis to \"map over\"\n",
    "    #   In this example, we essentially do\n",
    "    #   [(1 * 3 ), (2 * 3), (3 * 3)]\n",
    "    vmapped_fn_v1 = vmap(my_func, in_axes=(0, 0, 0))\n",
    "    res = vmapped_fn_v1(x, y, z)\n",
    "    assert jnp.all(\n",
    "        res == \n",
    "        x * 3\n",
    "    )\n",
    "    \n",
    "    # Alternatively, we can choose to not specify the axis\n",
    "    vmapped_fn_v2 = vmap(my_func)\n",
    "    res = vmapped_fn_v2(x, y, z)\n",
    "    assert jnp.all(\n",
    "        res == \n",
    "        x * 3\n",
    "    )\n",
    "    \n",
    "def vmap_broadcast():\n",
    "    def my_func(_x, _y, _z):\n",
    "        assert len(_x.shape) == 0\n",
    "        assert len(_y.shape) == 1 and _y.shape == (3,)\n",
    "        assert len(_z.shape) == 1 and _z.shape == (3,)\n",
    "        return _x * (_y + _z)\n",
    "\n",
    "    x = jnp.asarray([1, 2, 3])\n",
    "    y = jnp.asarray([1, 2, 3])\n",
    "    z = jnp.asarray([1, 2, 3])\n",
    "    \n",
    "    # Here, we specify \"None\" for y and z, which means that they are passed in \"as is\"\n",
    "    vmapped_fn = vmap(my_func, in_axes=(0, None, None))\n",
    "    res = vmapped_fn(x, y, z)\n",
    "    \n",
    "    # What's happening here is we pass in y and z as vectors, so we essentially \"broadcast\" and go up in a dimension\n",
    "    #   [(1 * (1,2,3) * 2), (2 * (1, 2, 3) * 2), (3 * (1, 2, 3) * 2)]\n",
    "    #   = ((2, 4, 6), (4, 8, 12), (6, 12, 18))\n",
    "    assert jnp.all(\n",
    "        res == \n",
    "        jnp.asarray([[2, 4, 6], [4, 8, 12], [6, 12, 18]])\n",
    "    )\n",
    "    assert res.shape == (3, 3)\n",
    "    \n",
    "    \n",
    "    \n",
    "vmap_on_all()\n",
    "vmap_broadcast()"
   ],
   "id": "651a7b7cce769283",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Real World Example: Batched Neural Network Inference\n",
    "\n",
    "And on to some real-world examples! Throughout the rest of the notebook, we'll see the \"core\" function implemented in two ways:\n",
    "\n",
    "`x_vmap` and `x`, where `x` is the way the function would be implemented in vectorized form, and the `x_vmap` is the vmap-ed form. Hopefully this illustrates how using the `vmap` more closely aligns our code with the math"
   ],
   "id": "d98b0923-f643-43a2-a5c5-cc01c81b6d79"
  },
  {
   "cell_type": "code",
   "id": "0ba6265c-dd7d-4d07-900c-7f70186431ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T16:42:33.427579Z",
     "start_time": "2024-09-26T16:42:33.365287Z"
    }
   },
   "source": [
    "# Define the neural network parameters\n",
    "W1 = jnp.array([[0.2, 0.4], [0.5, 0.3]])  # Shape (2, 2)\n",
    "W2 = jnp.array([0.6, 0.7])                # Shape (2,)\n",
    "\n",
    "# Example batched input data\n",
    "X_batch = jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])  # Shape (3, 2)\n",
    "\n",
    "# Activation function\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "# Single forward pass\n",
    "def forward_pass_vmap(x, W1, W2):\n",
    "    \"\"\"\n",
    "    TODO: Your code here\n",
    "    1) Take the dot product between W1 and x\n",
    "    2) apply the relu\n",
    "    3) return the dot product between W2 and the relu result\n",
    "    \"\"\"\n",
    "    assert x.shape == X_batch.shape[1:]\n",
    "    return W2 * relu(x * W1)\n",
    "\n",
    "def forward_pass(X, W1, W2):\n",
    "    dp = relu(jnp.dot(X, W1))\n",
    "    bla = jnp.dot(dp, W2)\n",
    "    print(dp.shape, W2.shape, bla.shape)\n",
    "    \n",
    "    return jnp.dot(dp, W2)\n",
    "    \n",
    "\n",
    "# Vectorized forward pass using vmap\n",
    "batched_forward_pass = vmap(forward_pass_vmap, in_axes=(0, None, None))\n",
    "vmap_batch_output = batched_forward_pass(X_batch, W1, W2)\n",
    "print(vmap_batch_output, vmap_batch_output.shape)\n",
    "\n",
    "vectorized_batch_output = forward_pass(X_batch, W1, W2)\n",
    "print(vectorized_batch_output)\n",
    "assert jnp.all(\n",
    "    vmap_batch_output == \n",
    "    vectorized_batch_output\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.12       0.56      ]\n",
      "  [0.3        0.42000002]]\n",
      "\n",
      " [[0.36       1.12      ]\n",
      "  [0.90000004 0.84000003]]\n",
      "\n",
      " [[0.6        1.6800001 ]\n",
      "  [1.5        1.26      ]]] (3, 2, 2)\n",
      "(3, 2) (2,) (3,)\n",
      "[1.4200001 3.24      5.0600004]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(3, 2, 2), (3,)]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/numpy_to_jax/lib/python3.12/site-packages/jax/_src/util.py:290\u001B[0m, in \u001B[0;36mcache.<locals>.wrap.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    289\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 290\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m cached(config\u001B[38;5;241m.\u001B[39mtrace_context(), \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/numpy_to_jax/lib/python3.12/site-packages/jax/_src/util.py:283\u001B[0m, in \u001B[0;36mcache.<locals>.wrap.<locals>.cached\u001B[0;34m(_, *args, **kwargs)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mlru_cache(max_size)\n\u001B[1;32m    282\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcached\u001B[39m(_, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 283\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/numpy_to_jax/lib/python3.12/site-packages/jax/_src/lax/lax.py:155\u001B[0m, in \u001B[0;36m_broadcast_shapes_cached\u001B[0;34m(*shapes)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;129m@cache\u001B[39m()\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_broadcast_shapes_cached\u001B[39m(\u001B[38;5;241m*\u001B[39mshapes: \u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mtuple\u001B[39m[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]:\n\u001B[0;32m--> 155\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _broadcast_shapes_uncached(\u001B[38;5;241m*\u001B[39mshapes)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/numpy_to_jax/lib/python3.12/site-packages/jax/_src/lax/lax.py:171\u001B[0m, in \u001B[0;36m_broadcast_shapes_uncached\u001B[0;34m(*shapes)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result_shape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 171\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncompatible shapes for broadcasting: shapes=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(shapes)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result_shape\n",
      "\u001B[0;31mValueError\u001B[0m: Incompatible shapes for broadcasting: shapes=[(3, 2, 2), (3,)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 39\u001B[0m\n\u001B[1;32m     36\u001B[0m vectorized_batch_output \u001B[38;5;241m=\u001B[39m forward_pass(X_batch, W1, W2)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(vectorized_batch_output)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m jnp\u001B[38;5;241m.\u001B[39mall(\n\u001B[0;32m---> 39\u001B[0m     vmap_batch_output \u001B[38;5;241m==\u001B[39m \n\u001B[1;32m     40\u001B[0m     vectorized_batch_output\n\u001B[1;32m     41\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/numpy_to_jax/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:264\u001B[0m, in \u001B[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m    262\u001B[0m args \u001B[38;5;241m=\u001B[39m (other, \u001B[38;5;28mself\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m swap \u001B[38;5;28;01melse\u001B[39;00m (\u001B[38;5;28mself\u001B[39m, other)\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(other, _accepted_binop_types):\n\u001B[0;32m--> 264\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m binary_op(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001B[39;00m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(other) \u001B[38;5;129;01min\u001B[39;00m _rejected_binop_types:\n",
      "    \u001B[0;31m[... skipping hidden 11 frame]\u001B[0m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/numpy_to_jax/lib/python3.12/site-packages/jax/_src/numpy/ufuncs.py:85\u001B[0m, in \u001B[0;36m_one_to_one_binop.<locals>.<lambda>\u001B[0;34m(x1, x2)\u001B[0m\n\u001B[1;32m     83\u001B[0m   fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x1, x2, \u001B[38;5;241m/\u001B[39m: lax_fn(\u001B[38;5;241m*\u001B[39mpromote_args_numeric(numpy_fn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, x1, x2))\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 85\u001B[0m   fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m x1, x2, \u001B[38;5;241m/\u001B[39m: lax_fn(\u001B[38;5;241m*\u001B[39mpromote_args(numpy_fn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, x1, x2))\n\u001B[1;32m     86\u001B[0m fn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjax.numpy.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnumpy_fn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     87\u001B[0m fn \u001B[38;5;241m=\u001B[39m jit(fn, inline\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/numpy_to_jax/lib/python3.12/site-packages/jax/_src/numpy/util.py:381\u001B[0m, in \u001B[0;36mpromote_args\u001B[0;34m(fun_name, *args)\u001B[0m\n\u001B[1;32m    379\u001B[0m _check_no_float0s(fun_name, \u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m    380\u001B[0m check_for_prngkeys(fun_name, \u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m--> 381\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m promote_shapes(fun_name, \u001B[38;5;241m*\u001B[39mpromote_dtypes(\u001B[38;5;241m*\u001B[39margs))\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/numpy_to_jax/lib/python3.12/site-packages/jax/_src/numpy/util.py:250\u001B[0m, in \u001B[0;36mpromote_shapes\u001B[0;34m(fun_name, *args)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39mnumpy_rank_promotion\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    249\u001B[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001B[0;32m--> 250\u001B[0m result_rank \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(lax\u001B[38;5;241m.\u001B[39mbroadcast_shapes(\u001B[38;5;241m*\u001B[39mshapes))\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [_broadcast_to(arg, (\u001B[38;5;241m1\u001B[39m,) \u001B[38;5;241m*\u001B[39m (result_rank \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(shp)) \u001B[38;5;241m+\u001B[39m shp)\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m arg, shp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(args, shapes)]\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/numpy_to_jax/lib/python3.12/site-packages/jax/_src/lax/lax.py:171\u001B[0m, in \u001B[0;36m_broadcast_shapes_uncached\u001B[0;34m(*shapes)\u001B[0m\n\u001B[1;32m    169\u001B[0m result_shape \u001B[38;5;241m=\u001B[39m _try_broadcast_shapes(shape_list)\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result_shape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 171\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncompatible shapes for broadcasting: shapes=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(shapes)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result_shape\n",
      "\u001B[0;31mValueError\u001B[0m: Incompatible shapes for broadcasting: shapes=[(3, 2, 2), (3,)]"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "a5e4af4f-422e-45b3-874f-ab1e483533d9",
   "metadata": {},
   "source": [
    "# Calculating the Gaussian PDF\n",
    "\n",
    "`gaussian_pdf_v` is the vmap-ed version of `gaussian_pdf`, which implements the function in vectorized form. You should study how the two are different\n",
    "and how this difference emerges because of the way the data is passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6292b5-f696-4408-9caf-f70066fbc78e",
   "metadata": {},
   "source": [
    "![](../assets/gaussian_pdf.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "60315ea0-0c54-4734-ad0f-d9fa0e06cfe1",
   "metadata": {},
   "source": [
    "mu = np.array([0, 0])  # Mean vector\n",
    "Sigma = np.array([[1, 0], [0, 1]])  # Covariance matrix\n",
    "X = np.array([[1, 1], [2, 2], [3, 3]])  # Point to evaluate the PDF\n",
    "\n",
    "# Arguments implicitly passed in. Done to keep the code cleaner for the example\n",
    "k = mu.shape[0]\n",
    "t1 = (2 * jnp.pi) ** (-k / 2)\n",
    "t2 = jnp.linalg.det(Sigma) ** (-0.5)\n",
    "inv = jnp.linalg.inv(Sigma)\n",
    "\n",
    "def gaussian_pdf_v(x_vec, mu_vec, Sigma):\n",
    "    \"\"\"\n",
    "    # TODO: implement the single-sample equivalent of `to_exp` in the gaussian_pdf/\n",
    "    #       the elements to be exponentiated in the image above\n",
    "    \"\"\"\n",
    "    diff = x_vec - mu_vec\n",
    "    to_exp = -0.5 * \n",
    "    to_exp = ... # TODO: Your code here! vmap(forward_pass, ...)\n",
    "    \n",
    "    return t1 * t2 * jnp.exp(to_exp)\n",
    "\n",
    "def gaussian_pdf(x_mat, mu_mat) -> np.array:\n",
    "    diff = x_mat - mu_mat\n",
    "    ###############################################################\n",
    "    to_exp = -0.5 * jnp.sum(diff @ inv * diff, axis=1)\n",
    "    ###############################################################\n",
    "    return t1 * t2 * jnp.exp(to_exp)\n",
    "\n",
    "\n",
    "vmapped_gaussian = vmap(gaussian_pdf_v, in_axes=(0, None, None))\n",
    "vmap_gauss_res = vmapped_gaussian(X, mu, Sigma)\n",
    "\n",
    "\n",
    "print(\"VMapped-Gaussian PDF correct?\", jnp.allclose(\n",
    "    vmap_gauss_res, \n",
    "    stats.multivariate_normal.pdf(X, mu, Sigma)\n",
    "))\n",
    "\n",
    "print(\"Typical-Gaussian PDF correct?\", jnp.allclose(\n",
    "    gaussian_pdf(X, mu, Sigma), \n",
    "    stats.multivariate_normal.pdf(X, mu, Sigma)\n",
    "))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4794415a-102c-4b9b-9553-afb6cece1445",
   "metadata": {},
   "source": [
    "# Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b4a24-e885-4741-8ec3-c8b3129ef074",
   "metadata": {},
   "source": [
    "![](../assets/softmax_regression.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "3a81ffd1-6b20-495e-a050-5e9907654135",
   "metadata": {},
   "source": [
    "# Example data\n",
    "X = jnp.array([[1, 2], [2, 3], [3, 4]])  # Batch of inputs\n",
    "W = jnp.array([[0.2, 0.8], [0.5, 0.1]])  # Weight matrix\n",
    "b = jnp.array([0.1, -0.2])  # Bias vector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38488af3-9e0e-48f9-af9f-63c0600ff96b",
   "metadata": {},
   "source": [
    "def softmax_regression(X, W, b):\n",
    "    logits = jnp.dot(X, W) + b\n",
    "    exp_logits = jnp.exp(logits)\n",
    "    probabilities = exp_logits / jnp.sum(exp_logits, axis=-1, keepdims=True)\n",
    "    return probabilities\n",
    "\n",
    "# Calculate softmax probabilities for the batch of inputs\n",
    "probabilities = softmax_regression(X, W, b)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5b9e321-84d7-4c6b-8fe6-fe6580134695",
   "metadata": {},
   "source": [
    "def single_softmax_regression(x, W, b):\n",
    "    \"\"\"\n",
    "    TODO: Implement the equivalent of the softmax_regression above on a single row of x\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "# Vectorize the single input calculation\n",
    "vectorized_softmax_regression = vmap(single_softmax_regression, in_axes=(0, None, None))\n",
    "\n",
    "# Calculate softmax probabilities using vmap\n",
    "probabilities_vmap = vectorized_softmax_regression(X, W, b)\n",
    "print(f\"Vmapped Softmax Regression equal to vectorized?: {np.allclose(probabilities, probabilities_vmap)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93f199f4-b886-4c9a-9b9e-7495c7f21463",
   "metadata": {},
   "source": [
    "# Further Exercises: \n",
    "\n",
    "## 1) Read up on [jax.pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html)\n",
    "\n",
    "`pmap` is a parallel map across devices and is useful for scaling across devices"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e9af2a0-8166-4cef-b538-db7722474301",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
