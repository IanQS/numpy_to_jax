{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055cedca-3ffa-4c17-ac28-23e69053af5e",
   "metadata": {},
   "source": [
    "# Gaussian_Mixture_Model\n",
    "\n",
    "We will be implementing the following algorithm:\n",
    "\n",
    "<img src=\"assets/gmm_alg.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69059dd2-012a-41bd-a466-0a8f9cf0c3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from sklearn.datasets import make_blobs\n",
    "import time\n",
    "\n",
    "\n",
    "from plot_util import plot_data, plot_training, confidence_ellipse\n",
    "from data_utils import make_ds, DatasetSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57828163-5c1f-448d-a64c-7cef353f9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUESS_NUM_CLASSES = 4\n",
    "\n",
    "unknown_centers = np.asarray([\n",
    "    [1, -1],  # bottom left\n",
    "    [5, 5],  # middle\n",
    "    [8, 7],  # mid-right\n",
    "    [10, 0]  # bottom right\n",
    "])\n",
    "\n",
    "# Options are: LARGE, MEDIUM, SMALL\n",
    "dataset_size = DatasetSize.LARGE\n",
    "\n",
    "\n",
    "(X, y), _ = make_ds(unknown_centers, dataset_size=dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bed8cb-4dc4-476b-a85f-ff2b58355f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, y, num_centers=len(unknown_centers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb86d71-0cac-4618-bb07-14530afc0432",
   "metadata": {},
   "source": [
    "# Diagnostic Functions\n",
    "\n",
    "The Gaussian Mixture Model is based on the maximum likelihood estimate, which optimizes for the log-likelihood. So, we should probably code it up and ensure that our model is optimizing the log-likelihood correctly.\n",
    "\n",
    "**Note** this is un-vectorized, but that's fine for our purposes, because we'll leverage jax to speed things up for us. \n",
    "\n",
    "**Note** this is step 4 (and 5, really) of the algorithm listed above\n",
    "\n",
    "\n",
    "![](assets/gaussian_pdf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d3e5e3-4a29-4af6-b5fd-a923aa69ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gaussian_pdf_cls_single(x, mu, sigma, pi):\n",
    "    lhs = 2 * jnp.pi ** (-len(mu) / 2)\n",
    "    mid = jnp.linalg.det(sigma) ** -0.5\n",
    "    \n",
    "    diff = x - mu\n",
    "    rhs = -0.5 * (diff @ jnp.linalg.inv(sigma) @ diff)\n",
    "    return lhs * mid * jnp.exp(rhs) * pi\n",
    "\n",
    "@jax.jit\n",
    "def log_likelihood(X, mus, sigmas, pis):\n",
    "    \"\"\"\n",
    "    Vectorized log-likelihood computation.\n",
    "    \"\"\"\n",
    "    _gaussian_pdf_cls_batch = jax.vmap(\n",
    "        fun=_gaussian_pdf_cls_single,\n",
    "        in_axes=(0, None, None, None)\n",
    "    )\n",
    "\n",
    "    gaussian_pdf_batch = jax.vmap(\n",
    "        fun=_gaussian_pdf_cls_batch,\n",
    "        in_axes=(None, 0, 0, 0)\n",
    "    )\n",
    "    pdfs = jnp.squeeze(gaussian_pdf_batch(X, mus, sigmas, pis)).T\n",
    "    \n",
    "    # Sum PDFs across classes for each data point, then take log and sum\n",
    "    return jnp.sum(jnp.log(jnp.sum(pdfs, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f85fe0-c7e2-4983-8f97-77565ce1e5fb",
   "metadata": {},
   "source": [
    "# E-step\n",
    "\n",
    "The \"Expectation\" step of the Expectation-Maximization algorithm, where we calculate the \"expected-ness\" of hte data\n",
    "\n",
    "<img src=\"assets/e_step.png\" width=\"500\">\n",
    "\n",
    "**Note** the image above was taken (with permission) from [Prof. Matt Golub](https://homes.cs.washington.edu/~mgolub/)'s course, [Machine Learning for Neuroscience (CSE599N)](https://courses.cs.washington.edu/courses/cse599n/24sp/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87711618-d794-469c-a963-edaece6ae3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def e_step(X, mus, sigmas, pis):\n",
    "    # Get PDFs for all data points and classes\n",
    "    _gaussian_pdf_cls_batch = jax.vmap(\n",
    "        fun=_gaussian_pdf_cls_single,\n",
    "        in_axes=(0, None, None, None)\n",
    "    )\n",
    "    gaussian_pdf_batch = jax.vmap(\n",
    "        fun=_gaussian_pdf_cls_batch,\n",
    "        in_axes=(None, 0, 0, 0)\n",
    "    )\n",
    "    pdfs = jnp.squeeze(gaussian_pdf_batch(X, mus, sigmas, pis)).T\n",
    "    # Normalize to get responsibilities\n",
    "    # Sum across classes for each data point\n",
    "    pdf_sums = jnp.sum(pdfs, axis=1, keepdims=True)\n",
    "    responsibilities = pdfs / pdf_sums\n",
    "    \n",
    "    return responsibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df56970-00da-43ff-affc-d140b0f756e1",
   "metadata": {},
   "source": [
    "# M-step\n",
    "\n",
    "The \"maximixation\" step of the Expectation-Maximization algorithm, where we do actual optimization\n",
    "\n",
    "<img src=\"assets/m_step.png\" width=\"500\">\n",
    "\n",
    "**Note** the image above was taken (with permission) from [Prof. Matt Golub](https://homes.cs.washington.edu/~mgolub/)'s course, [Machine Learning for Neuroscience (CSE599N)](https://courses.cs.washington.edu/courses/cse599n/24sp/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b8a4e4-0e10-4198-b9e7-a59055e61b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_params_single_class(X, resp, cls_resp_sum, mu, sigma, pi):\n",
    "    cls_resp = jnp.expand_dims(resp, axis=-1)\n",
    "    scale_factor = 1 / cls_resp_sum\n",
    "    new_mus = scale_factor * jnp.sum(cls_resp * X, axis=0)\n",
    "\n",
    "    deviation = X - mu\n",
    "    scaled_deviation = cls_resp * deviation\n",
    "    new_sigmas = scale_factor * (scaled_deviation.T @ deviation)\n",
    "    new_pis = cls_resp_sum\n",
    "\n",
    "    return new_mus, new_sigmas, new_pis\n",
    "    \n",
    "def m_step(X, responsibilities, mus, sigmas, pis):\n",
    "    cls_resp_sum = jnp.sum(responsibilities, axis=0)\n",
    "\n",
    "    update_params_single_class = jax.vmap(\n",
    "        fun=_update_params_single_class,\n",
    "        in_axes=(None, 1, 0, 0, 0, 0)\n",
    "    )\n",
    "    new_mus, new_sigmas, new_pis = update_params_single_class(\n",
    "        X, responsibilities, cls_resp_sum, mus, sigmas, pis\n",
    "    )\n",
    "    return new_mus, new_sigmas, new_pis / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687ee72-fb44-4868-b769-4ee453929862",
   "metadata": {},
   "source": [
    "# Initialize the Guesses \n",
    "\n",
    "- Step 1 of the original algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62042e1-aa33-45c3-a803-5e79817f7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_guesses(X, guessed_num_classes, dataset_size):\n",
    "    # We just say the covariance of the entire dataset is the covariance of each sub-cluster.\n",
    "    sigmas = np.asarray([np.cov(X.T) for _ in range(guessed_num_classes)])\n",
    "    \n",
    "    # We simply \n",
    "    cls_probs = np.expand_dims(\n",
    "        np.asarray([1 / guessed_num_classes for _ in range(guessed_num_classes)]).T,\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    _, mus = make_ds(unknown_centers, dataset_size)\n",
    "\n",
    "    mus = jnp.asarray(mus)\n",
    "    sigmas = jnp.asarray(sigmas)\n",
    "    cls_probs = jnp.asarray(cls_probs)\n",
    "    return mus, sigmas, cls_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8939aef4-0520-4d06-8f00-59c668330a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "def gmm(\n",
    "    X: np.ndarray,\n",
    "    guess_num_classes,\n",
    "    dataset_size,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "    mus, sigmas, cls_probs = initialize_guesses(X, guess_num_classes, dataset_size)\n",
    "    counter = 0\n",
    "    ll_container = []\n",
    "    TOL = 0.0001\n",
    "    ll_container.append(np.inf)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    while True:  # Run until converges\n",
    "        # e-step\n",
    "        responsibilities = e_step(X, mus, sigmas, cls_probs)\n",
    "\n",
    "        # m-step\n",
    "        mus, sigmas, cls_probs = m_step(X, responsibilities, mus, sigmas, cls_probs)\n",
    "        # Recalculate the log-likelihood\n",
    "        ll_curr = float(log_likelihood(X, mus, sigmas, cls_probs))\n",
    "\n",
    "        if np.abs(ll_container[-1] - ll_curr) < TOL:\n",
    "            print(f\"Converged to within {TOL} after: {counter} iterations\")\n",
    "            break\n",
    "\n",
    "        ll_container.append(float(ll_curr))\n",
    "        if verbose and counter % 5 == 0 and counter > 0:\n",
    "            print(f\"Data Log-Likelihood at iteration: {counter} = {ll_curr:.6f}\")\n",
    "        counter += 1\n",
    "\n",
    "    responsibilities = e_step(X, mus, sigmas, cls_probs)\n",
    "    print(f\"Total Training time was: {time.perf_counter() - start_time:.4f}s over {counter} rounds\")\n",
    "    return mus, sigmas, cls_probs.T, responsibilities.T, ll_container[1:]\n",
    "    # -------------------------- #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238a0c5-9b12-420b-84cb-42ce9659a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mus, sigmas, cls_priors, _, lls = gmm(\n",
    "    X,\n",
    "    guess_num_classes=GUESS_NUM_CLASSES,\n",
    "    dataset_size=dataset_size,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4422bce-ad0c-4f04-820d-c3ea4200c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83bb82-ae16-490d-9231-71f393f2a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "colors = [\"r\", \"g\", \"b\", \"y\"]\n",
    "\n",
    "for i, c in enumerate(colors):\n",
    "    \n",
    "    # Plot the centers\n",
    "    plt.scatter(unknown_centers[i, 0], unknown_centers[i, 1], c=c, marker=\"o\", label=f\"Cluster: {i} True Center\")\n",
    "    plt.scatter(mus[i, 0], mus[i, 1], c=c, marker=\"^\", label=f\"Cluster: {i} Inferred Center\")\n",
    "    \n",
    "    # Plot the standard deviations\n",
    "    mask = y == i\n",
    "    masked_points = X[mask]\n",
    "    mu_x = np.mean(masked_points, axis=0)\n",
    "    sigma = np.cov(masked_points[:, 0], masked_points[:, 1])\n",
    "    confidence_ellipse(mu_x, sigma,  ax=axs, n_std=1, edgecolor=c, linestyle=\"-\")\n",
    "    confidence_ellipse(mu_x, sigma, ax=axs, n_std=2, edgecolor=c, linestyle=\"-\")\n",
    "    confidence_ellipse(mu_x, sigma, ax=axs, n_std=3, edgecolor=c, linestyle=\"-\")\n",
    "\n",
    "\n",
    "    confidence_ellipse(mus[i], sigmas[i],  ax=axs, n_std=1, edgecolor=c, linestyle=\"--\")\n",
    "    confidence_ellipse(mus[i], sigmas[i], ax=axs, n_std=2, edgecolor=c, linestyle=\"--\")\n",
    "    confidence_ellipse(mus[i], sigmas[i], ax=axs, n_std=3, edgecolor=c, linestyle=\"--\")\n",
    "plt.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c24de-9f8e-439d-9882-dc6acc8700f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
