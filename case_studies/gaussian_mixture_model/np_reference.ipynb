{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4dff0f5c3e9f47",
   "metadata": {},
   "source": [
    "# Gaussian_Mixture_Model\n",
    "\n",
    "We will be implementing the following algorithm:\n",
    "\n",
    "<img src=\"assets/gmm_alg.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd4cf1134f4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from sklearn.datasets import make_blobs\n",
    "import time\n",
    "\n",
    "\n",
    "from plot_util import plot_data, plot_training, confidence_ellipse\n",
    "from data_utils import make_ds, DatasetSize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd4a99-25e4-4862-8d00-d07cdfb9b1aa",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "Say we are trying to model a simple problem where we have 105 points that are 2-d. We know that there are roughly 4 clusters* and we know more-or-less where they start. However, we want to learn the parameters to fit them.\n",
    "\n",
    "*In the next notebook we do not assume that we have this prior knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faad402-3901-498b-8fef-cad8b6c69239",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUESS_NUM_CLASSES = 4\n",
    "\n",
    "unknown_centers = np.asarray([\n",
    "    [1, -1],  # bottom left\n",
    "    [5, 5],  # middle\n",
    "    [8, 7],  # mid-right\n",
    "    [10, 0]  # bottom right\n",
    "])\n",
    "\n",
    "# Options are: LARGE, MEDIUM, SMALL\n",
    "dataset_size = DatasetSize.LARGE\n",
    "\n",
    "\n",
    "(X, y), _ = make_ds(unknown_centers, dataset_size=dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b844db0a-f4d5-4a7b-914e-b49104e08692",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, y, num_centers=len(unknown_centers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1bbaf5-093a-4e57-a6b5-90327ed694a2",
   "metadata": {},
   "source": [
    "# Diagnostic Functions\n",
    "\n",
    "The Gaussian Mixture Model is based on the maximum likelihood estimate, which optimizes for the log-likelihood. So, we should probably code it up and ensure that our model is optimizing the log-likelihood correctly.\n",
    "\n",
    "**Note** this is un-vectorized, but that's fine for our purposes, because we'll leverage jax to speed things up for us. \n",
    "\n",
    "**Note** this is step 4 (and 5, really) of the algorithm listed above\n",
    "\n",
    "\n",
    "![](assets/gaussian_pdf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50924d34-b734-4342-8478-6c87629d88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(X, mus, sigmas, pis):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = len(mus)\n",
    "    \n",
    "    det_sigmas = np.linalg.det(sigmas)\n",
    "    inv_sigmas = np.linalg.inv(sigmas)\n",
    "    \n",
    "    norm_constants = (2 * np.pi) ** (-n_features/2) * det_sigmas ** (-0.5)\n",
    "    \n",
    "    pdfs = np.zeros((n_samples, n_classes))\n",
    "    \n",
    "    for cls_idx in range(n_classes):\n",
    "        diff = X - mus[cls_idx] \n",
    "\n",
    "        scaled = diff @ inv_sigmas[cls_idx] \n",
    "        quad = np.sum(scaled * diff, axis=1)\n",
    "        \n",
    "        pdfs[:, cls_idx] = norm_constants[cls_idx] * np.exp(-0.5 * quad) * pis[cls_idx]\n",
    "    \n",
    "    return pdfs\n",
    "\n",
    "\n",
    "def log_likelihood(X, mus, sigmas, pis):\n",
    "    \"\"\"\n",
    "    Vectorized log-likelihood computation.\n",
    "    \"\"\"\n",
    "    pdfs = gaussian_pdf(X, mus, sigmas, pis)\n",
    "    # Sum PDFs across classes for each data point, then take log and sum\n",
    "    return np.sum(np.log(np.sum(pdfs, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45966e-8d3b-41e5-b821-abe8749d40bf",
   "metadata": {},
   "source": [
    "# E-step\n",
    "\n",
    "The \"Expectation\" step of the Expectation-Maximization algorithm, where we calculate the \"expected-ness\" of hte data\n",
    "\n",
    "<img src=\"assets/e_step.png\" width=\"500\">\n",
    "\n",
    "**Note** the image above was taken (with permission) from [Prof. Matt Golub](https://homes.cs.washington.edu/~mgolub/)'s course, [Machine Learning for Neuroscience (CSE599N)](https://courses.cs.washington.edu/courses/cse599n/24sp/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e5b83-0284-4916-a391-071a2f1dbf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(X, mus, sigmas, pis):\n",
    "    # Get PDFs for all data points and classes\n",
    "    pdfs = gaussian_pdf(X, mus, sigmas, pis)\n",
    "    # Normalize to get responsibilities\n",
    "    # Sum across classes for each data point\n",
    "    pdf_sums = np.sum(pdfs, axis=1, keepdims=True)\n",
    "    responsibilities = pdfs / pdf_sums\n",
    "    \n",
    "    return responsibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15193915-3cd6-4f78-bd5f-5e8e1b339b99",
   "metadata": {},
   "source": [
    "# M-step\n",
    "\n",
    "The \"maximixation\" step of the Expectation-Maximization algorithm, where we do actual optimization\n",
    "\n",
    "<img src=\"assets/m_step.png\" width=\"500\">\n",
    "\n",
    "**Note** the image above was taken (with permission) from [Prof. Matt Golub](https://homes.cs.washington.edu/~mgolub/)'s course, [Machine Learning for Neuroscience (CSE599N)](https://courses.cs.washington.edu/courses/cse599n/24sp/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d16469-6d45-48b0-b9d9-420fff6d3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(X, responsibilities, mus, sigmas, pis):\n",
    "    cls_resp_sum = np.sum(responsibilities, axis=0)\n",
    "    num_cls = len(mus)\n",
    "\n",
    "    new_mus = np.zeros_like(mus, dtype=np.float32)\n",
    "    new_sigmas = np.zeros_like(sigmas, dtype=np.float32)\n",
    "    new_pis = np.zeros_like(pis, dtype=np.float32)\n",
    "\n",
    "    for cls_idx in range(num_cls):\n",
    "        cls_resp = np.expand_dims(responsibilities[:, cls_idx], axis=-1)\n",
    "        scale_factor = 1 / cls_resp_sum[cls_idx]\n",
    "        new_mus[cls_idx] = scale_factor * np.sum(cls_resp * X, axis=0)\n",
    "\n",
    "        deviation = X - mus[cls_idx]\n",
    "        scaled_deviation = cls_resp * deviation\n",
    "        new_sigmas[cls_idx] = scale_factor * (scaled_deviation.T @ deviation)\n",
    "        new_pis[cls_idx] = cls_resp_sum[cls_idx] / len(X)\n",
    "        \n",
    "    return new_mus, new_sigmas, new_pis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b78f08-2256-4b2c-8133-3d0b5c738b5d",
   "metadata": {},
   "source": [
    "# Initialize the Guesses \n",
    "\n",
    "- Step 1 of the original algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d1e0e5-2fd6-48cd-a2ec-3b6c7a3c3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_guesses(X, guessed_num_classes, dataset_size):\n",
    "    # We just say the covariance of the entire dataset is the covariance of each sub-cluster.\n",
    "    sigmas = np.asarray([np.cov(X.T) for _ in range(guessed_num_classes)])\n",
    "    \n",
    "    # We simply \n",
    "    cls_probs = np.expand_dims(\n",
    "        np.asarray([1 / guessed_num_classes for _ in range(guessed_num_classes)]).T,\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "    _, mus = make_ds(unknown_centers, dataset_size)\n",
    "    mus = mus.astype(np.float32)\n",
    "    sigmas = sigmas.astype(np.float32)\n",
    "    cls_probs = cls_probs.astype(np.float32)\n",
    "\n",
    "    return mus, sigmas, cls_probs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56037539-b7c6-41d7-b2f5-c0b0ae7a258a",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Assemble the final algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ced69c-0006-4e9e-a262-e8e8704aa2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm(\n",
    "    X: np.ndarray,\n",
    "    guess_num_classes,\n",
    "    dataset_size,\n",
    "    verbose=False\n",
    "):\n",
    "\n",
    "    mus, sigmas, cls_probs = initialize_guesses(X, guess_num_classes, dataset_size)\n",
    "    counter = 0\n",
    "    ll_container = []\n",
    "    TOL = 0.0001\n",
    "    ll_container.append(np.inf)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    while True:  # Run until converges\n",
    "        # e-step\n",
    "        responsibilities = e_step(X, mus, sigmas, cls_probs)\n",
    "\n",
    "        # m-step\n",
    "        mus, sigmas, cls_probs = m_step(X, responsibilities, mus, sigmas, cls_probs)\n",
    "\n",
    "        # Recalculate the log-likelihood\n",
    "        ll_curr = float(log_likelihood(X, mus, sigmas, cls_probs))\n",
    "\n",
    "        if np.abs(ll_container[-1] - ll_curr) < TOL:\n",
    "            print(f\"Converged to within {TOL} after: {counter} iterations\")\n",
    "            break\n",
    "\n",
    "        ll_container.append(float(ll_curr))\n",
    "        if verbose and counter % 5 == 0 and counter > 0:\n",
    "            print(f\"Data Log-Likelihood at iteration: {counter} = {ll_curr:.6f}\")\n",
    "        counter += 1\n",
    "\n",
    "    responsibilities = e_step(X, mus, sigmas, cls_probs)\n",
    "    print(f\"Total Training time was: {time.perf_counter() - start_time:.4f}s over {counter} rounds\")\n",
    "    return mus, sigmas, cls_probs.T, responsibilities.T, ll_container[1:]\n",
    "    # -------------------------- #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12057ea1-902b-4eb1-bee6-94f93671901c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mus, sigmas, cls_priors, _, lls = gmm(\n",
    "    X,\n",
    "    guess_num_classes=GUESS_NUM_CLASSES,\n",
    "    dataset_size=dataset_size,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de0807-6c34-4dd5-88cc-229741f0b77e",
   "metadata": {},
   "source": [
    "# Visualize Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05fcfd-57c0-42ac-838e-95be356fc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(lls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a2751-a1ab-4e31-93e2-dac561436b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(15, 10))\n",
    "\n",
    "colors = [\"r\", \"g\", \"b\", \"y\"]\n",
    "\n",
    "for i, c in enumerate(colors):\n",
    "    \n",
    "    # Plot the centers\n",
    "    plt.scatter(unknown_centers[i, 0], unknown_centers[i, 1], c=c, marker=\"o\", label=f\"Cluster: {i} True Center\")\n",
    "    plt.scatter(mus[i, 0], mus[i, 1], c=c, marker=\"^\", label=f\"Cluster: {i} Inferred Center\")\n",
    "    \n",
    "    # Plot the standard deviations\n",
    "    mask = y == i\n",
    "    masked_points = X[mask]\n",
    "    mu_x = np.mean(masked_points, axis=0)\n",
    "    sigma = np.cov(masked_points[:, 0], masked_points[:, 1])\n",
    "    confidence_ellipse(mu_x, sigma,  ax=axs, n_std=1, edgecolor=c, linestyle=\"-\")\n",
    "    confidence_ellipse(mu_x, sigma, ax=axs, n_std=2, edgecolor=c, linestyle=\"-\")\n",
    "    confidence_ellipse(mu_x, sigma, ax=axs, n_std=3, edgecolor=c, linestyle=\"-\")\n",
    "\n",
    "\n",
    "    confidence_ellipse(mus[i], sigmas[i],  ax=axs, n_std=1, edgecolor=c, linestyle=\"--\")\n",
    "    confidence_ellipse(mus[i], sigmas[i], ax=axs, n_std=2, edgecolor=c, linestyle=\"--\")\n",
    "    confidence_ellipse(mus[i], sigmas[i], ax=axs, n_std=3, edgecolor=c, linestyle=\"--\")\n",
    "plt.legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26368251-6ff9-4765-9e3d-7b8d952d1254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
