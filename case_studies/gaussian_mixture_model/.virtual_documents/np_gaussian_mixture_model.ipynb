


import matplotlib.pyplot as plt
import numpy as np
np.random.seed(123)
from sklearn.datasets import make_blobs
import time


from plot_util import plot_data, plot_training, confidence_ellipse





unknown_centers = np.asarray([
    [1, -1],  # bottom left
    [5, 5],  # middle
    [8, 7],  # mid-right
    [10, 0]  # bottom right
])


def make_ds(centers):
    points_in_classes = [300, 250, 200, 40]
    ################################################
    # Initial Guesses
    ################################################
    # Randomly increase/ decrease by 25% each way
    scale = (np.random.randint(low=-80, high=80, size=centers.shape)) / 100

    initial_mu_guesses = centers + (centers * scale)
    return make_blobs(points_in_classes, centers=centers), initial_mu_guesses

(X, y), _ = make_ds(unknown_centers)


plot_data(X, y, num_centers=len(unknown_centers))





def gaussian_pdf_cls(x, mus, sigmas, pis) -> np.ndarray:
    # mus, sigmas, and pis must have the same leading axis length
    # To reuse this function later on in the E-step, we save to a list instead of summing the values up directly
    num_classes = len(mus)
    x_lls = np.zeros((num_classes))
    data_dim = len(x)
    for cls_idx in range(num_classes):
        mu, sigma, pi = mus[cls_idx], sigmas[cls_idx], pis[cls_idx]

        lhs = (2 * np.pi ** -data_dim/2) * np.linalg.det(sigma) ** -0.5
        sig_inv = np.linalg.inv(sigma)
        logit_rhs = -0.5 * (x - mu).T @ sig_inv @ (x - mu)
        cls_pdf = (lhs * np.exp(logit_rhs)) * np.squeeze(pi)
        x_lls[cls_idx] = cls_pdf
    return x_lls
        
def log_likelihood(X, mus, sigmas, pis):
    ll = 0
    for i in range(len(X)):
        ll += np.log(np.sum(gaussian_pdf_cls(X[i], mus, sigmas, pis)))
    return ll






def e_step(X, mus, sigmas, pis):
    num_cls = len(mus)
    responsibilities = np.zeros((len(X), num_cls))
    for point_idx in range(len(X)):
        lls = gaussian_pdf_cls(X[point_idx], mus, sigmas, pis)
        denom = np.sum(lls)
        for resp_idx in range(num_cls):            
            responsibilities[point_idx][resp_idx] = lls[resp_idx] / denom
    return responsibilities





def m_step(X, responsibilities, mus, sigmas, pis):
    cls_resp_sum = np.sum(responsibilities, axis=0)
    num_cls = len(mus)

    new_mus = np.zeros_like(mus)
    new_sigmas = np.zeros_like(sigmas)
    new_pis = np.zeros_like(pis)

    for cls_idx in range(num_cls):
        cls_resp = np.expand_dims(responsibilities[:, cls_idx], axis=-1)
        scale_factor = 1 / cls_resp_sum[cls_idx]
        new_mus[cls_idx] = scale_factor * np.sum(cls_resp * X, axis=0)

        deviation = X - mus[cls_idx]
        scaled_deviation = cls_resp * deviation
        new_sigmas[cls_idx] = scale_factor * (scaled_deviation.T @ deviation)
        new_pis[cls_idx] = cls_resp_sum[cls_idx] / len(X)
        
    return new_mus, new_sigmas, new_pis





def initialize_guesses(X, guessed_num_classes):
    # We just say the covariance of the entire dataset is the covariance of each sub-cluster.
    sigmas = np.asarray([np.cov(X.T) for _ in range(guessed_num_classes)])
    
    # We simply 
    cls_probs = np.expand_dims(
        np.asarray([1 / guessed_num_classes for _ in range(guessed_num_classes)]).T,
        axis=-1
    )

    _, mus = make_ds(unknown_centers)

    return mus, sigmas, cls_probs
    






def gmm(
    X: np.ndarray,
    guess_num_classes,
    verbose=False
):

    mus, sigmas, cls_probs = initialize_guesses(X, guess_num_classes)
    counter = 0
    ll_container = []
    TOL = 0.00001
    ll_container.append(np.inf)

    start_time = time.perf_counter()
    while True:  # Run until converges
        # e-step
        responsibilities = e_step(X, mus, sigmas, cls_probs)

        # m-step
        mus, sigmas, cls_probs = m_step(X, responsibilities, mus, sigmas, cls_probs)
        # Recalculate the log-likelihood
        ll_curr = float(log_likelihood(X, mus, sigmas, cls_probs))

        if np.abs(ll_container[-1] - ll_curr) < TOL:
            print(f"Converged to within {TOL} after: {counter} iterations")
            break

        ll_container.append(float(ll_curr))
        if verbose and counter % 2 == 0 and counter > 0:
            print(f"Data Log-Likelihood at iteration: {counter} = {ll_curr:.6f}")
        counter += 1

    responsibilities = e_step(X, mus, sigmas, cls_probs)
    print(f"Total Training time was: {time.perf_counter() - start_time:.4f}s")
    return mus, sigmas, cls_probs.T, responsibilities.T, ll_container[1:]
    # -------------------------- #




GUESS_NUM_CLASSES = 4
mus, sigmas, cls_priors, _, lls = gmm(
    X,
    guess_num_classes=GUESS_NUM_CLASSES,
    verbose=True
)





plot_training(lls)


fig, axs = plt.subplots(1, 1, figsize=(15, 10))

colors = ["r", "g", "b", "y"]

for i, c in enumerate(colors):
    
    # Plot the centers
    plt.scatter(unknown_centers[i, 0], unknown_centers[i, 1], c=c, marker="o", label=f"Cluster: {i} True Center")
    plt.scatter(mus[i, 0], mus[i, 1], c=c, marker="^", label=f"Cluster: {i} Inferred Center")
    
    # Plot the standard deviations
    mask = y == i
    masked_points = X[mask]
    mu_x = np.mean(masked_points, axis=0)
    sigma = np.cov(masked_points[:, 0], masked_points[:, 1])
    confidence_ellipse(mu_x, sigma,  ax=axs, n_std=1, edgecolor=c, linestyle="-")
    confidence_ellipse(mu_x, sigma, ax=axs, n_std=2, edgecolor=c, linestyle="-")
    confidence_ellipse(mu_x, sigma, ax=axs, n_std=3, edgecolor=c, linestyle="-")


    confidence_ellipse(mus[i], sigmas[i],  ax=axs, n_std=1, edgecolor=c, linestyle="--")
    confidence_ellipse(mus[i], sigmas[i], ax=axs, n_std=2, edgecolor=c, linestyle="--")
    confidence_ellipse(mus[i], sigmas[i], ax=axs, n_std=3, edgecolor=c, linestyle="--")
plt.legend(loc="best")




